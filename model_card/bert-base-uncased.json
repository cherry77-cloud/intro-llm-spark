{
   "BertModel": {
       "embeddings": {
           "word_embeddings": "Embedding(30522, 768, padding_idx=0)",
           "position_embeddings": "Embedding(512, 768)",
           "token_type_embeddings": "Embedding(2, 768)",
           "LayerNorm": "LayerNorm(768, eps=1e-12, elementwise_affine=True)",
           "dropout": "Dropout(p=0.1, inplace=False)"
       },
       "encoder": {
           "layer": [
               "12 x BertLayer",
               {
                   "attention": {
                       "self": {
                           "query": "Linear(768, 768, bias=True)",
                           "key": "Linear(768, 768, bias=True)",
                           "value": "Linear(768, 768, bias=True)",
                           "dropout": "Dropout(p=0.1, inplace=False)"
                       },
                       "output": {
                           "dense": "Linear(768, 768, bias=True)",
                           "LayerNorm": "LayerNorm(768, eps=1e-12, elementwise_affine=True)",
                           "dropout": "Dropout(p=0.1, inplace=False)"
                       }
                   },
                   "intermediate": {
                       "dense": "Linear(768, 3072, bias=True)",
                       "intermediate_act_fn": "GELUActivation()"
                   },
                   "output": {
                       "dense": "Linear(3072, 768, bias=True)",
                       "LayerNorm": "LayerNorm(768, eps=1e-12, elementwise_affine=True)",
                       "dropout": "Dropout(p=0.1, inplace=False)"
                   }
               }
           ]
       },
       "pooler": {
           "dense": "Linear(768, 768, bias=True)",
           "activation": "Tanh()"
       }
   }
}
