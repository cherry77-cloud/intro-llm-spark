ğŸ“ æ–‡æœ¬è¾“å…¥ "Hello world!"
    â†“
ğŸ”ª æ­£åˆ™åˆ†è¯ ["Hello", " world", "!"]  
    â†“
ğŸ”„ é€tokenå¤„ç†ï¼š
    â”œâ”€ UTF-8ç¼–ç : "Hello" â†’ [72,101,108,108,111]
    â”œâ”€ å­—èŠ‚æ˜ å°„: bytes â†’ unicodeå®‰å…¨å­—ç¬¦
    â”œâ”€ BPEåˆå¹¶: æŒ‰ä¼˜å…ˆçº§è¿­ä»£åˆå¹¶å­—ç¬¦å¯¹
    â””â”€ IDè½¬æ¢: BPE tokens â†’ æ•°å­—ID
    â†“
ğŸ¯ è¾“å‡º token IDs [15496, 995, 0]

è§£ç æ—¶å®Œå…¨é€†å‘æ“ä½œï¼šIDs â†’ BPE â†’ bytes â†’ UTF-8 â†’ åŸæ–‡æœ¬


class Encoder:
   """GPT-2 å­—èŠ‚çº§ BPE ç¼–ç å™¨
   æ ¸å¿ƒåŠŸèƒ½ï¼šå°†ä»»æ„æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„ token ID åºåˆ—
   å·¥ä½œåŸç†ï¼šæ–‡æœ¬ â†’ æ­£åˆ™åˆ†è¯ â†’ å­—èŠ‚çº§ Unicode æ˜ å°„ â†’ BPE åˆå¹¶ â†’ token ID
   """
   
   def __init__(self, encoder, bpe_merges, errors='replace'):
       """åˆå§‹åŒ–ç¼–ç å™¨
       
       å·¥ä½œæµç¨‹ï¼š
           1. æ„å»ºåŒå‘æ˜ å°„è¡¨ï¼štoken â†” ID, byte â†” unicode
           2. å»ºç«‹ BPE ä¼˜å…ˆçº§æ’åºï¼šæ—©æœŸåˆå¹¶è§„åˆ™ä¼˜å…ˆçº§æ›´é«˜
           3. ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼šç”¨äºæ™ºèƒ½åˆ†è¯
           4. åˆå§‹åŒ–ç¼“å­˜ï¼šé¿å…é‡å¤ BPE è®¡ç®—
       """
       self.encoder = encoder                    # token â†’ ID æ˜ å°„
       self.decoder = {v:k for k,v in encoder.items()}  # ID â†’ token æ˜ å°„
       self.byte_encoder = bytes_to_unicode()    # byte â†’ unicode æ˜ å°„
       self.byte_decoder = {v:k for k,v in self.byte_encoder.items()}  # åå‘æ˜ å°„
       self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))  # åˆå¹¶ä¼˜å…ˆçº§
       self.cache = {}                          # BPE ç»“æœç¼“å­˜
       # æ­£åˆ™æ¨¡å¼ï¼šå¤„ç†ç¼©å†™ã€å­—æ¯ã€æ•°å­—ã€æ ‡ç‚¹ã€ç©ºç™½
       self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")

   def bpe(self, token):
       """å¯¹å•ä¸ª token æ‰§è¡Œå­—èŠ‚å¯¹ç¼–ç 
       
       ç®—æ³•æµç¨‹ï¼š
           1. æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
           2. å°† token è½¬ä¸ºå­—ç¬¦å…ƒç»„
           3. è¿­ä»£åˆå¹¶ï¼š
              - æ‰¾å‡ºæ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹
              - é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„å¯¹è¿›è¡Œåˆå¹¶
              - æ›´æ–° tokenï¼Œé‡æ–°è®¡ç®—å­—ç¬¦å¯¹
              - é‡å¤ç›´åˆ°æ— æ³•ç»§ç»­åˆå¹¶
           4. ç¼“å­˜ç»“æœå¹¶è¿”å›
       """

   def encode(self, text):
       """å°†æ–‡æœ¬ç¼–ç ä¸º token ID åºåˆ—
       """

   def decode(self, tokens):
       """å°† token ID åºåˆ—è§£ç ä¸ºæ–‡æœ¬
       
       é€†å‘å·¥ä½œæµç¨‹ï¼š
           è¾“å…¥: [15496, 995, 0, 20204]
           1. ID è½¬ token: [15496, 995, 0, 20204] â†’ ["Hello", "Ä world", "!", "Ä ä½ å¥½"]
           2. æ‹¼æ¥ BPE tokens: "HelloÄ world!Ä ä½ å¥½"
           3. Unicode è½¬å­—èŠ‚: æ¯ä¸ªå­—ç¬¦ â†’ å¯¹åº”å­—èŠ‚å€¼
           4. UTF-8 è§£ç : å­—èŠ‚æ•°ç»„ â†’ "Hello world! ä½ å¥½"


def top_k_logits(logits, k):
    if k == 0:
        return logits
    
    def _top_k():
        values, _ = tf.nn.top_k(logits, k=k)    # æ ¸å¿ƒæ“ä½œï¼šè·å–å‰ k ä¸ªæœ€å¤§çš„ logit å€¼
        min_values = values[:, -1, tf.newaxis]  # è·å–ç¬¬ k å¤§çš„å€¼ä½œä¸ºæˆªæ–­é˜ˆå€¼
        return tf.where(
            logits < min_values,           # æ¡ä»¶ï¼šlogitå€¼ < ç¬¬kå¤§çš„å€¼
            tf.ones_like(logits) * -1e10,
            logits,                       
        )
    
    # åŠ¨æ€æ¡ä»¶æ‰§è¡Œï¼šé¿å…ä¸å¿…è¦çš„è®¡ç®—
    return tf.cond(
        tf.equal(k, 0),     # å¦‚æœk=0
        lambda: logits,     # ç›´æ¥è¿”å›åŸlogits
        lambda: _top_k(),   # å¦åˆ™æ‰§è¡Œtop-kæˆªæ–­
    )


def top_p_logits(logits, p):
    """Nucleus sampling - åŠ¨æ€æˆªæ–­ç­–ç•¥"""
    batch, _ = logits.shape.as_list()
    
    # æ­¥éª¤1ï¼šæŒ‰æ¦‚ç‡ä»å¤§åˆ°å°æ’åº
    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)
    
    # æ­¥éª¤2ï¼šè®¡ç®—æ’åºåçš„ç´¯ç§¯æ¦‚ç‡åˆ†å¸ƒ
    cumulative_probs = tf.cumsum(
        tf.nn.softmax(sorted_logits, axis=-1), axis=-1
    )
    
    # æ­¥éª¤3ï¼šæ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡åˆšå¥½â‰¤pçš„æœ€åä¸€ä¸ªä½ç½®
    indices = tf.stack([
        tf.range(0, batch),  # æ‰¹æ¬¡ç´¢å¼• [0, 1, 2, ..., batch_size-1]
        # å…³é”®è®¡ç®—ï¼šæœ‰å¤šå°‘ä¸ªtokençš„ç´¯ç§¯æ¦‚ç‡â‰¤p
        tf.maximum(
            tf.reduce_sum(tf.cast(cumulative_probs <= p, tf.int32), axis=-1) - 1, 
            0
        ),
    ], axis=-1)
    
    # æ­¥éª¤4ï¼šè·å–æˆªæ–­é˜ˆå€¼
    min_values = tf.gather_nd(sorted_logits, indices)
    
    # æ­¥éª¤5ï¼šåº”ç”¨æˆªæ–­
    return tf.where(
        logits < min_values,           # åŸå§‹logit < é˜ˆå€¼
        tf.ones_like(logits) * -1e10,  # è®¾ä¸ºè´Ÿæ— ç©·
        logits,                        # ä¿æŒåŸå€¼
    )

# æ¸©åº¦ç¼©æ”¾ï¼ˆTemperature Scalingï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# é€šè¿‡é™¤ä»¥æ¸©åº¦å‚æ•°Tæ¥è°ƒèŠ‚æ¦‚ç‡åˆ†å¸ƒçš„"å°–é”ç¨‹åº¦"
#   â€¢ T > 1.0 â†’ logitsç¼©å° â†’ æ¦‚ç‡åˆ†å¸ƒæ›´å¹³æ»‘ â†’ å¢åŠ éšæœºæ€§å’Œåˆ›é€ åŠ›
#   â€¢ T < 1.0 â†’ logitsæ”¾å¤§ â†’ æ¦‚ç‡åˆ†å¸ƒæ›´å°–é” â†’ è¶‹å‘ç¡®å®šæ€§é€‰æ‹©  
#   â€¢ T = 1.0 â†’ ä¿æŒåŸå§‹åˆ†å¸ƒä¸å˜
# æ•°å­¦åŸç†ï¼šP(token_i) = exp(logit_i/T) / Î£ exp(logit_j/T)
logits = next_outputs['logits'][:, -1, :] / tf.to_float(temperature)

# â‘¡ Top-K æˆªæ–­é‡‡æ ·ï¼ˆTop-K Samplingï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åªä¿ç•™æ¦‚ç‡æœ€é«˜çš„å‰kä¸ªå€™é€‰tokenï¼Œå…¶ä½™å…¨éƒ¨å±è”½ï¼š
#   â€¢ æ‰¾å‡ºå‰kä¸ªæœ€å¤§çš„logitå€¼ï¼Œä»¥ç¬¬kå¤§å€¼ä¸ºé˜ˆå€¼
#   â€¢ å°†å°äºé˜ˆå€¼çš„ä½ç½®è®¾ä¸º-1e10ï¼ˆç›¸å½“äºæ¦‚ç‡â‰ˆ0ï¼‰
#   â€¢ k=0è¡¨ç¤ºä¸è¿›è¡Œæˆªæ–­ï¼Œä¿ç•™æ‰€æœ‰å€™é€‰
# ä½œç”¨ï¼šé˜²æ­¢æ¨¡å‹é€‰æ‹©æ˜æ˜¾ä¸åˆç†çš„ä½æ¦‚ç‡è¯æ±‡
logits = top_k_logits(logits, k=top_k)

# â‘¢ Top-P/æ ¸é‡‡æ ·ï¼ˆNucleus Samplingï¼‰  
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åŠ¨æ€é€‰æ‹©ç´¯ç§¯æ¦‚ç‡åˆšå¥½è¾¾åˆ°pçš„æœ€å°tokené›†åˆï¼š
#   â€¢ å°†tokenæŒ‰æ¦‚ç‡ä»é«˜åˆ°ä½æ’åº
#   â€¢ æ‰¾åˆ°ä½¿ç´¯ç§¯æ¦‚ç‡ â‰¥ p çš„æœ€å°å‰ç¼€é›†åˆ
#   â€¢ åªä¿ç•™è¿™ä¸ª"æ ¸å¿ƒå€™é€‰é›†"ï¼Œå…¶ä½™è®¾ä¸º-1e10
#   â€¢ p=1.0è¡¨ç¤ºä¸è¿›è¡Œæˆªæ–­
# ä¼˜åŠ¿ï¼šæ ¹æ®æ¦‚ç‡åˆ†å¸ƒå½¢çŠ¶è‡ªé€‚åº”è°ƒæ•´å€™é€‰æ•°é‡
logits = top_p_logits(logits, p=top_p)

# å¤šé¡¹å¼éšæœºé‡‡æ ·ï¼ˆMultinomial Samplingï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä»ç»è¿‡ç­›é€‰çš„å€™é€‰æ± ä¸­æŒ‰æ¦‚ç‡éšæœºé€‰æ‹©ä¸€ä¸ªtokenï¼š
#   â€¢ TensorFlowå†…éƒ¨å…ˆå¯¹logitsè¿›è¡Œsoftmaxå½’ä¸€åŒ–
#   â€¢ ç„¶åæŒ‰å¤šé¡¹å¼åˆ†å¸ƒéšæœºæŠ½æ ·num_samples=1ä¸ªtoken
#   â€¢ æ¯æ¬¡è°ƒç”¨ç»“æœå¯èƒ½ä¸åŒï¼Œä½“ç°çœŸæ­£çš„éšæœºæ€§
# æ›¿ä»£æ–¹æ¡ˆï¼štf.argmaxå®ç°è´ªå¿ƒè§£ç ï¼ˆæ€»æ˜¯é€‰æ¦‚ç‡æœ€é«˜çš„ï¼‰
samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)


def body(past, prev, output):
    # ã€è‡ªå›å½’æ ¸å¿ƒã€‘ï¼šåŸºäºå½“å‰åºåˆ—é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
    next_outputs = step(hparams, prev, past=past)
    logits = next_outputs['logits'][:, -1, :]  # åªå–æœ€åä½ç½®çš„é¢„æµ‹
    
    # ... é‡‡æ ·ç­–ç•¥ ...
    
    # ã€å…³é”®çš„è‡ªå›å½’çŠ¶æ€æ›´æ–°ã€‘ï¼š
    return [
        updated_past,                              # å†å²çŠ¶æ€ç´¯ç§¯
        samples,                                   # å½“å‰è¾“å‡º â†’ ä¸‹ä¸€æ­¥è¾“å…¥
        tf.concat([output, samples], axis=1)       # åºåˆ—é€æ­¥æ„å»º
    ]

_, _, tokens = tf.while_loop(
    cond=cond,                       # å¾ªç¯æ¡ä»¶
    body=body,                       # æ¯æ¬¡è¿­ä»£æ‰§è¡Œçš„è‡ªå›å½’æ­¥éª¤
    maximum_iterations=length-1,     # ç”Ÿæˆlength-1ä¸ªæ–°token
    loop_vars=[past, prev, output],  # å¾ªç¯å˜é‡ï¼šå†å²ã€å½“å‰ã€è¾“å‡º
)


def multihead_attn(q, k, v):
    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šæ¯ä¸ªä½ç½®å¯¹å…¶ä»–ä½ç½®çš„"å…³æ³¨åº¦"
    w = tf.matmul(q, k, transpose_b=True)  # [batch, heads, seq_len, seq_len]
    w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))  # ç¼©æ”¾é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
    
    # ã€å…³é”®ã€‘å› æœæ©ç ï¼šç¡®ä¿åªèƒ½çœ‹åˆ°"è¿‡å»"ï¼Œä¸èƒ½çœ‹åˆ°"æœªæ¥"
    w = mask_attn_weights(w)  # ä¸Šä¸‰è§’è®¾ä¸º-âˆï¼Œä¸‹ä¸‰è§’ä¿ç•™
    w = softmax(w)            # è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    
    # åŠ æƒèšåˆï¼šåŸºäºæ³¨æ„åŠ›æƒé‡æ•´åˆä¿¡æ¯
    a = tf.matmul(w, v)       # æœ€ç»ˆçš„ä¸Šä¸‹æ–‡è¡¨ç¤º
    return a

def mask_attn_weights(w):
    # ç”Ÿæˆå› æœæ©ç ï¼šä½ç½®iåªèƒ½å…³æ³¨ä½ç½®â‰¤içš„ä¿¡æ¯
    b = attention_mask(nd, ns, dtype=w.dtype)  # ä¸‹ä¸‰è§’çŸ©é˜µ
    w = w*b - tf.cast(1e10, w.dtype)*(1-b)    # æœªæ¥ä½ç½®è®¾ä¸ºæå°å€¼
    return w


def attn(x, scope, n_state, *, past, hparams):
    # â‘  è®¡ç®—å½“å‰æ­¥çš„Qã€Kã€V
    c = conv1d(x, 'c_attn', n_state*3)
    q, k, v = map(split_heads, tf.split(c, 3, axis=2))
    
    # ã€æ ¸å¿ƒä¼˜åŒ–ã€‘ä¿å­˜å½“å‰çš„Kã€Vç”¨äºä¸‹æ¬¡ç¼“å­˜
    present = tf.stack([k, v], axis=1)  # æ–°çš„ç¼“å­˜çŠ¶æ€
    
    # ã€æ•ˆç‡å…³é”®ã€‘å¦‚æœæœ‰å†å²ç¼“å­˜ï¼Œç›´æ¥æ‹¼æ¥ä½¿ç”¨
    if past is not None:
        pk, pv = tf.unstack(past, axis=1)  # å–å‡ºç¼“å­˜çš„Kã€V
        k = tf.concat([pk, k], axis=-2)    # å†å²K + å½“å‰K
        v = tf.concat([pv, v], axis=-2)    # å†å²V + å½“å‰V

    a = multihead_attn(q, k, v)
    a = merge_heads(a)
    a = conv1d(a, 'c_proj', n_state)
    return a, present
